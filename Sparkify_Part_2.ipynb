{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7f9ce8d",
   "metadata": {},
   "source": [
    "# DATA SCIENTIST NANODEGREE - SPARKIFY CAPSTONE PROJECT\n",
    "## Part 2 - Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06c8887",
   "metadata": {},
   "source": [
    "<img src=\"images/udacity-logo.png\" alt=\"udacity logo\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35f67ea",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In Part 1 of this project, we performed analysis of user event log dataset for the fictitious Sparkify service. The outcome of that was a train and test dataset containing engineered features. These datasets can be used to model Machine Learning models.\n",
    "<img src='images/sparkify.png' alt='sparkify logo'/>\n",
    "In Part 2, we will do just that. Using the train and test data, we will attempt to use ML algorithms supported by PySpark to model the data and use it to predict users who are likely to churn in the near future.\n",
    "\n",
    "For modelling a supervised Machine Learning algorithm, we require the data to be rows of individual objects,users in this case, and several features as columns and also an additional label column. Let's investigate if the data in hand conforms to this structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e74bf9",
   "metadata": {},
   "source": [
    "## Structure of this Notebook\n",
    "- Importing and inspecting the train and test datasets\n",
    "- Apply preprocessing require for numerical and categoric features and creating a pipeline using the preprocessing steps\n",
    "- Model selection\n",
    "- Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a95aef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from utility_functions import *\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer,MinMaxScaler, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('max_colwidth', None)\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddadab0",
   "metadata": {},
   "source": [
    "## Importing and inspecting the train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b21e6074",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[userId: string, churned: double, avg_num_of_add_to_playlist_per_session: double, avg_num_of_addfriends_per_session: double, avg_num_of_adverts_per_session: double, avg_num_of_artists_per_session: double, avg_num_of_songs_per_session: double, avg_num_of_thumbs_down_per_session: double, avg_num_of_thumbs_up_per_session: double, avg_num_of_times_settings_changed_per_session: double, average_number_of_visits_to_the_about_page_per_session: double, average_number_of_visits_to_the_help_page_per_session: double, avg_num_of_visits_to_home_per_session: double, avg_num_of_visits_to_the_settings_page_per_session: double, avg_num_of_visits_to_upgrade_page: double, avg_number_of_errors_per_session: double, avg_number_of_visits_to_downgrade_page: double, num_times_user_changed_levels: bigint, num_of_downgrades_submitted: bigint, num_of_upgrades_submitted: bigint, gender: string]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing train and test dataset\n",
    "train_file_path = \"train_data.parquet\"\n",
    "test_file_path = \"test_data.parquet\"\n",
    "\n",
    "train_data = import_data_into_dataframe(train_file_path, 'parquet')\n",
    "test_data = import_data_into_dataframe(test_file_path, 'parquet')\n",
    "\n",
    "# caching the datasets\n",
    "train_data.persist()\n",
    "test_data.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80c04281",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 21 columns and 357 rows in the train data set\n",
      "The train data set has information about 357 users\n",
      "Out of which 276 are non churners and 81 are churners\n",
      "There are 21 columns and 91 rows in the test data set\n",
      "The test data set has information about 91 users\n",
      "Out of which 73 are non churners and 18 are churners\n"
     ]
    }
   ],
   "source": [
    "# using a utility function to summarize the data sets\n",
    "print_data_summary(train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db78b61",
   "metadata": {},
   "source": [
    "From the summary, we can see that this is an <b>imbalanced dataset</b>. There are about three times more non-churnes compared to churners. And this is a problem for ML algorithms and they have little information about churners to learn. As a result, the model can end up with bad performance. \n",
    "\n",
    "Also, we have to be careful of what metric we use to measure the performance. For a dataset like this, we cannot use accuracy as a measure. We will resort to using the <b>AUC-ROC score</b> as our metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14043b19",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: string (nullable = true)\n",
      " |-- churned: double (nullable = true)\n",
      " |-- avg_num_of_add_to_playlist_per_session: double (nullable = true)\n",
      " |-- avg_num_of_addfriends_per_session: double (nullable = true)\n",
      " |-- avg_num_of_adverts_per_session: double (nullable = true)\n",
      " |-- avg_num_of_artists_per_session: double (nullable = true)\n",
      " |-- avg_num_of_songs_per_session: double (nullable = true)\n",
      " |-- avg_num_of_thumbs_down_per_session: double (nullable = true)\n",
      " |-- avg_num_of_thumbs_up_per_session: double (nullable = true)\n",
      " |-- avg_num_of_times_settings_changed_per_session: double (nullable = true)\n",
      " |-- average_number_of_visits_to_the_about_page_per_session: double (nullable = true)\n",
      " |-- average_number_of_visits_to_the_help_page_per_session: double (nullable = true)\n",
      " |-- avg_num_of_visits_to_home_per_session: double (nullable = true)\n",
      " |-- avg_num_of_visits_to_the_settings_page_per_session: double (nullable = true)\n",
      " |-- avg_num_of_visits_to_upgrade_page: double (nullable = true)\n",
      " |-- avg_number_of_errors_per_session: double (nullable = true)\n",
      " |-- avg_number_of_visits_to_downgrade_page: double (nullable = true)\n",
      " |-- num_times_user_changed_levels: long (nullable = true)\n",
      " |-- num_of_downgrades_submitted: long (nullable = true)\n",
      " |-- num_of_upgrades_submitted: long (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Schema of the train data\n",
    "train_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2da41939",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: string (nullable = true)\n",
      " |-- churned: double (nullable = true)\n",
      " |-- avg_num_of_add_to_playlist_per_session: double (nullable = true)\n",
      " |-- avg_num_of_addfriends_per_session: double (nullable = true)\n",
      " |-- avg_num_of_adverts_per_session: double (nullable = true)\n",
      " |-- avg_num_of_artists_per_session: double (nullable = true)\n",
      " |-- avg_num_of_songs_per_session: double (nullable = true)\n",
      " |-- avg_num_of_thumbs_down_per_session: double (nullable = true)\n",
      " |-- avg_num_of_thumbs_up_per_session: double (nullable = true)\n",
      " |-- avg_num_of_times_settings_changed_per_session: double (nullable = true)\n",
      " |-- average_number_of_visits_to_the_about_page_per_session: double (nullable = true)\n",
      " |-- average_number_of_visits_to_the_help_page_per_session: double (nullable = true)\n",
      " |-- avg_num_of_visits_to_home_per_session: double (nullable = true)\n",
      " |-- avg_num_of_visits_to_the_settings_page_per_session: double (nullable = true)\n",
      " |-- avg_num_of_visits_to_upgrade_page: double (nullable = true)\n",
      " |-- avg_number_of_errors_per_session: double (nullable = true)\n",
      " |-- avg_number_of_visits_to_downgrade_page: double (nullable = true)\n",
      " |-- num_times_user_changed_levels: long (nullable = true)\n",
      " |-- num_of_downgrades_submitted: long (nullable = true)\n",
      " |-- num_of_upgrades_submitted: long (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Schema of the test data\n",
    "test_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5062968",
   "metadata": {},
   "source": [
    "<b>Both train and test have the same schema. The churned column indicates whether the user churned from the Sparkify service. All columns after the churned column are engineered features.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3e447e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'userId': 0,\n",
       " 'churned': 0,\n",
       " 'avg_num_of_add_to_playlist_per_session': 0,\n",
       " 'avg_num_of_addfriends_per_session': 0,\n",
       " 'avg_num_of_adverts_per_session': 0,\n",
       " 'avg_num_of_artists_per_session': 0,\n",
       " 'avg_num_of_songs_per_session': 0,\n",
       " 'avg_num_of_thumbs_down_per_session': 0,\n",
       " 'avg_num_of_thumbs_up_per_session': 0,\n",
       " 'avg_num_of_times_settings_changed_per_session': 0,\n",
       " 'average_number_of_visits_to_the_about_page_per_session': 0,\n",
       " 'average_number_of_visits_to_the_help_page_per_session': 0,\n",
       " 'avg_num_of_visits_to_home_per_session': 0,\n",
       " 'avg_num_of_visits_to_the_settings_page_per_session': 0,\n",
       " 'avg_num_of_visits_to_upgrade_page': 0,\n",
       " 'avg_number_of_errors_per_session': 0,\n",
       " 'avg_number_of_visits_to_downgrade_page': 0,\n",
       " 'num_times_user_changed_levels': 0,\n",
       " 'num_of_downgrades_submitted': 0,\n",
       " 'num_of_upgrades_submitted': 0,\n",
       " 'gender': 0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking for null values in the train data\n",
    "count_null_values_for_each_column(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6537f83",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'userId': 0,\n",
       " 'churned': 0,\n",
       " 'avg_num_of_add_to_playlist_per_session': 0,\n",
       " 'avg_num_of_addfriends_per_session': 0,\n",
       " 'avg_num_of_adverts_per_session': 0,\n",
       " 'avg_num_of_artists_per_session': 0,\n",
       " 'avg_num_of_songs_per_session': 0,\n",
       " 'avg_num_of_thumbs_down_per_session': 0,\n",
       " 'avg_num_of_thumbs_up_per_session': 0,\n",
       " 'avg_num_of_times_settings_changed_per_session': 0,\n",
       " 'average_number_of_visits_to_the_about_page_per_session': 0,\n",
       " 'average_number_of_visits_to_the_help_page_per_session': 0,\n",
       " 'avg_num_of_visits_to_home_per_session': 0,\n",
       " 'avg_num_of_visits_to_the_settings_page_per_session': 0,\n",
       " 'avg_num_of_visits_to_upgrade_page': 0,\n",
       " 'avg_number_of_errors_per_session': 0,\n",
       " 'avg_number_of_visits_to_downgrade_page': 0,\n",
       " 'num_times_user_changed_levels': 0,\n",
       " 'num_of_downgrades_submitted': 0,\n",
       " 'num_of_upgrades_submitted': 0,\n",
       " 'gender': 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking for null values in the test data\n",
    "count_null_values_for_each_column(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31debdf1",
   "metadata": {},
   "source": [
    "<b>There are no null values. This is a result of the way the dataset was created where missing values of all features were filled with zeros.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdde751f",
   "metadata": {},
   "source": [
    "## Apply preprocessing required for numerical and categoric features and creating a pipeline using the preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a11e211d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Train data set for correctness:\n",
      "     type  count\n",
      "0  bigint      3\n",
      "1  double     16\n",
      "2  string      2\n",
      "----------------------------------------\n",
      "Checking Test data set for correctness:\n",
      "     type  count\n",
      "0  bigint      3\n",
      "1  double     16\n",
      "2  string      2\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking Train data set for correctness:\")\n",
    "print(count_column_types(train_data).iloc[:, :2])\n",
    "print('-'*40)\n",
    "print(\"Checking Test data set for correctness:\")\n",
    "print(count_column_types(test_data).iloc[:, :2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3cb205d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['num_times_user_changed_levels',\n",
       " 'num_of_downgrades_submitted',\n",
       " 'num_of_upgrades_submitted',\n",
       " 'avg_num_of_add_to_playlist_per_session',\n",
       " 'avg_num_of_addfriends_per_session',\n",
       " 'avg_num_of_adverts_per_session',\n",
       " 'avg_num_of_artists_per_session',\n",
       " 'avg_num_of_songs_per_session',\n",
       " 'avg_num_of_thumbs_down_per_session',\n",
       " 'avg_num_of_thumbs_up_per_session',\n",
       " 'avg_num_of_times_settings_changed_per_session',\n",
       " 'average_number_of_visits_to_the_about_page_per_session',\n",
       " 'average_number_of_visits_to_the_help_page_per_session',\n",
       " 'avg_num_of_visits_to_home_per_session',\n",
       " 'avg_num_of_visits_to_the_settings_page_per_session',\n",
       " 'avg_num_of_visits_to_upgrade_page',\n",
       " 'avg_number_of_errors_per_session',\n",
       " 'avg_number_of_visits_to_downgrade_page']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create list of numeric column names, have to remove the churned column itself\n",
    "numeric_column_names = get_columns_of_type(train_data, \"bigint\")\n",
    "numeric_column_names.extend(get_columns_of_type(train_data, \"double\"))\n",
    "numeric_column_names.remove('churned') #remove churned column\n",
    "numeric_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f72db1ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gender']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a list of categoric column names and remove userId from it \n",
    "categoric_column_names = get_columns_of_type(train_data, \"string\")\n",
    "categoric_column_names.remove('userId')\n",
    "categoric_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bb5be74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we perform the necessary preprocessing, we will add each step as a stage of a pipeline\n",
    "pipeline_stages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c680496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling numeric columns\n",
    "numeric_vec_assembler = VectorAssembler(inputCols=numeric_column_names, outputCol=\"numeric_features\") # create a vector of all the numeric columns\n",
    "pipeline_stages.append(numeric_vec_assembler)\n",
    "\n",
    "minmax_scaler = MinMaxScaler(inputCol=\"numeric_features\", outputCol=\"numeric_features_scaled\") # minmax scale all numeric columns\n",
    "pipeline_stages.append(minmax_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f98e3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding the categorical column as gender is binary in this case\n",
    "str_indexer = StringIndexer(inputCols=categoric_column_names, outputCols=[name+'_indexed' for name in categoric_column_names], handleInvalid='skip') # encode all categorical features\n",
    "pipeline_stages.append(str_indexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e993aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a vector of all the features\n",
    "feature_columns = [\"numeric_features_scaled\", \"gender_indexed\"]\n",
    "feature_assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\") # create the combined features vector\n",
    "pipeline_stages.append(feature_assembler)\n",
    "\n",
    "output_label_indexer = StringIndexer(inputCol='churned', outputCol='label') # encode the churned column\n",
    "pipeline_stages.append(output_label_indexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21afc97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|churned|features                                                                                                                                                                                                                                                                                                   |label|\n",
      "+-------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|1.0    |[0.0,0.0,0.0,0.25,0.3213793103448276,0.676923076923077,0.16524590163934424,0.14317548746518105,0.125,0.29411764705882354,0.0,0.0,0.4175,0.22666666666666666,0.0,0.14285714285714285,0.4,0.0,1.0]                                                                                                           |1.0  |\n",
      "|0.0    |[0.16666666666666666,0.0,0.3333333333333333,0.25,0.5062068965517241,0.19230769230769232,0.28662295081967215,0.2864345403899721,0.23125,0.3205882352941177,0.3333333333333333,0.46799999999999997,0.3225,0.28200000000000003,0.2783333333333333,0.14285714285714285,0.4,0.44,1.0]                           |0.0  |\n",
      "|0.0    |[0.16666666666666666,0.0,0.3333333333333333,0.31785714285714284,0.43310344827586206,0.2561538461538462,0.27672131147540985,0.266100278551532,0.25,0.5176470588235295,0.3333333333333333,0.4,0.25,0.314,0.3333333333333333,0.2857142857142857,0.4,0.7272727272727273,0.0]                                   |0.0  |\n",
      "|1.0    |[0.3333333333333333,0.3333333333333333,0.3333333333333333,0.3364285714285714,0.4482758620689655,0.22153846153846155,0.379344262295082,0.3604178272980501,0.35375,0.5623529411764706,0.3333333333333333,0.48,0.5,0.3333333333333333,0.30833333333333335,0.14285714285714285,0.5,0.5290909090909092,1.0]     |1.0  |\n",
      "|0.0    |[0.3333333333333333,0.3333333333333333,0.3333333333333333,0.19357142857142856,0.2524137931034483,0.21153846153846156,0.12829508196721312,0.11233983286908078,0.1875,0.18941176470588236,0.3333333333333333,0.4,0.25,0.14666666666666667,0.1883333333333333,0.14285714285714285,0.4,0.30363636363636365,0.0]|0.0  |\n",
      "+-------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[userId: string, churned: double, avg_num_of_add_to_playlist_per_session: double, avg_num_of_addfriends_per_session: double, avg_num_of_adverts_per_session: double, avg_num_of_artists_per_session: double, avg_num_of_songs_per_session: double, avg_num_of_thumbs_down_per_session: double, avg_num_of_thumbs_up_per_session: double, avg_num_of_times_settings_changed_per_session: double, average_number_of_visits_to_the_about_page_per_session: double, average_number_of_visits_to_the_help_page_per_session: double, avg_num_of_visits_to_home_per_session: double, avg_num_of_visits_to_the_settings_page_per_session: double, avg_num_of_visits_to_upgrade_page: double, avg_number_of_errors_per_session: double, avg_number_of_visits_to_downgrade_page: double, num_times_user_changed_levels: bigint, num_of_downgrades_submitted: bigint, num_of_upgrades_submitted: bigint, gender: string, numeric_features: vector, numeric_features_scaled: vector, gender_indexed: double, features: vector, label: double]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tranforming the train data set using the pipeline\n",
    "# we will use this transformed data to train the model before adding the model itself as the final stage\n",
    "data_pipeline = Pipeline(stages=pipeline_stages)\n",
    "data_pipeline_model = data_pipeline.fit(train_data)\n",
    "transformed_data = temp_model.transform(train_data)\n",
    "transformed_data.select(\"churned\", \"features\", \"label\").show(5, truncate=False)\n",
    "transformed_data.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a79f047",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cae29f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate different algorithms\n",
    "model_names = ['logistic regression', 'random forest', 'gradient-boosted tree', 'linear svc',\n",
    "               'decision tree', 'naive bayes']\n",
    "estimators = []\n",
    "\n",
    "lr = LogisticRegression()\n",
    "estimators.append(lr)\n",
    "\n",
    "rf = RandomForestClassifier(seed = 42)\n",
    "estimators.append(rf)\n",
    "\n",
    "gbt = GBTClassifier(seed = 42)\n",
    "estimators.append(gbt)\n",
    "\n",
    "svc = LinearSVC()\n",
    "estimators.append(svc)\n",
    "\n",
    "dt = DecisionTreeClassifier(seed = 42)\n",
    "estimators.append(dt)\n",
    "\n",
    "nb = NaiveBayes()\n",
    "estimators.append(nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5946ccfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm: logistic regression\n",
      "Cross validation score: 0.6065959019718983\n",
      "AUCROC: 0.7153784219001608\n",
      "\n",
      "Algorithm: random forest\n",
      "Cross validation score: 0.5990755120479732\n",
      "AUCROC: 0.9315172660583285\n",
      "\n",
      "Algorithm: gradient-boosted tree\n",
      "Cross validation score: 0.5207247125103985\n",
      "AUCROC: 0.9997316156736447\n",
      "\n",
      "Algorithm: linear svc\n",
      "Cross validation score: 0.6033980599829254\n",
      "AUCROC: 0.6823224190373949\n",
      "\n",
      "Algorithm: decision tree\n",
      "Cross validation score: 0.5244351974131821\n",
      "AUCROC: 0.40915190552871716\n",
      "\n",
      "Algorithm: naive bayes\n",
      "Cross validation score: 0.500353935878279\n",
      "AUCROC: 0.5078278761853641\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to evaluate base line models, we will use cross validation with the default number of folds \n",
    "# the evaluation metric used by BinaryClassificationEvaluator is AUCROC, a number ranging from 0 to 1\n",
    "# a value of 1 means the model has great classification separation capacity\n",
    "# a value of 0 also means the model has great classification separation capacity, however the predicted labels are flipped.\n",
    "# a value near 0.5 means the model has no separation capacity at all\n",
    "\n",
    "best_metric_value = -99\n",
    "best_model = None\n",
    "\n",
    "for model_name, est in zip(model_names,estimators):\n",
    "    \n",
    "    evaluator = BinaryClassificationEvaluator() # using a binary classification evalution with metric as AUCROC\n",
    "    \n",
    "    grid = ParamGridBuilder().build() # using an empty grid\n",
    "    \n",
    "    crossval = CrossValidator(estimator = est,\n",
    "                            estimatorParamMaps=grid,\n",
    "                            evaluator = evaluator) # using the default value for number of folds: 3\n",
    "    \n",
    "    cvmodel = crossval.fit(transformed_data)\n",
    "    \n",
    "    metric_val = evaluator.evaluate(cvmodel.transform(transformed_data))\n",
    "    \n",
    "    print(f\"Algorithm: {model_name}\")\n",
    "    print(f\"Cross validation score: {cvmodel.avgMetrics[0]}\")\n",
    "    print(f\"AUCROC: {metric_val}\")\n",
    "    print()\n",
    "    \n",
    "    if metric_val > best_metric_value:\n",
    "        best_metric_value = metric_val\n",
    "        best_model = cvmodel.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aca1a7b",
   "metadata": {},
   "source": [
    "<b>As it can be seen, logistic regression had the best average cross validation score. Followed by Linear SVC.\n",
    "We will tune hyperparameters for the logistic regression model.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "604e180d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(estimator, evaluator, paramGrid, data):\n",
    "    crossval = CrossValidator(estimator = est,\n",
    "                            estimatorParamMaps=grid,\n",
    "                            evaluator = evaluator) # using the default value for number of folds: 3\n",
    "    \n",
    "    cvmodel = crossval.fit(data)\n",
    "    \n",
    "    return cvmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "31facbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will tune the maximum iterations, regularization parameter and the classification threshold parameters of the logistic regression\n",
    "# algorithm\n",
    "\n",
    "est = LogisticRegression()\n",
    "\n",
    "grid = ParamGridBuilder().addGrid(est.maxIter, [100, 200, 300]).addGrid(est.regParam, [0.001, 0.01, 0.1, 1, 3, 5]).addGrid(est.threshold, [0.3, 0.4, 0.5, 0.6, 0.7, 0.8]).build()\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "lr = train_classifier(est, evaluator, grid, transformed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9967cdfa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation score: 0.6184790292341781\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cross validation score: {lr.avgMetrics[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966cda8c",
   "metadata": {},
   "source": [
    "<b>Even after fine tuning, the average score only slighlty improved.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5023375c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxIter:  100\n",
      "threshold:  0.3\n",
      "regParam:  0.01\n"
     ]
    }
   ],
   "source": [
    "# the parameters of the best performing model\n",
    "print(\"maxIter: \",lr.bestModel.getMaxIter())\n",
    "print(\"threshold: \", lr.bestModel.getThreshold())\n",
    "print(\"regParam: \", lr.bestModel.getRegParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "62c00d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.24864945, -0.10793773, -0.25570171,  1.03269487, -0.49079552,\n",
       "        2.30837552,  0.46266292,  0.79770161,  1.70916452, -1.33829473,\n",
       "       -1.29602326, -0.72130076,  0.92615866, -0.20009427,  0.05294991,\n",
       "       -0.04719589, -1.50679953,  1.85935604,  0.29654608])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.bestModel.coefficients.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0106f79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = numeric_column_names + categoric_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7794890e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>LR coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>num_times_user_changed_levels</td>\n",
       "      <td>-0.248649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>num_of_downgrades_submitted</td>\n",
       "      <td>-0.107938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>num_of_upgrades_submitted</td>\n",
       "      <td>-0.255702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>avg_num_of_add_to_playlist_per_session</td>\n",
       "      <td>1.032695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>avg_num_of_addfriends_per_session</td>\n",
       "      <td>-0.490796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>avg_num_of_adverts_per_session</td>\n",
       "      <td>2.308376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>avg_num_of_artists_per_session</td>\n",
       "      <td>0.462663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>avg_num_of_songs_per_session</td>\n",
       "      <td>0.797702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>avg_num_of_thumbs_down_per_session</td>\n",
       "      <td>1.709165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>avg_num_of_thumbs_up_per_session</td>\n",
       "      <td>-1.338295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>avg_num_of_times_settings_changed_per_session</td>\n",
       "      <td>-1.296023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>average_number_of_visits_to_the_about_page_per_session</td>\n",
       "      <td>-0.721301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>average_number_of_visits_to_the_help_page_per_session</td>\n",
       "      <td>0.926159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>avg_num_of_visits_to_home_per_session</td>\n",
       "      <td>-0.200094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>avg_num_of_visits_to_the_settings_page_per_session</td>\n",
       "      <td>0.052950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>avg_num_of_visits_to_upgrade_page</td>\n",
       "      <td>-0.047196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>avg_number_of_errors_per_session</td>\n",
       "      <td>-1.506800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>avg_number_of_visits_to_downgrade_page</td>\n",
       "      <td>1.859356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>gender</td>\n",
       "      <td>0.296546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   feature  LR coefficient\n",
       "0                            num_times_user_changed_levels       -0.248649\n",
       "1                              num_of_downgrades_submitted       -0.107938\n",
       "2                                num_of_upgrades_submitted       -0.255702\n",
       "3                   avg_num_of_add_to_playlist_per_session        1.032695\n",
       "4                        avg_num_of_addfriends_per_session       -0.490796\n",
       "5                           avg_num_of_adverts_per_session        2.308376\n",
       "6                           avg_num_of_artists_per_session        0.462663\n",
       "7                             avg_num_of_songs_per_session        0.797702\n",
       "8                       avg_num_of_thumbs_down_per_session        1.709165\n",
       "9                         avg_num_of_thumbs_up_per_session       -1.338295\n",
       "10           avg_num_of_times_settings_changed_per_session       -1.296023\n",
       "11  average_number_of_visits_to_the_about_page_per_session       -0.721301\n",
       "12   average_number_of_visits_to_the_help_page_per_session        0.926159\n",
       "13                   avg_num_of_visits_to_home_per_session       -0.200094\n",
       "14      avg_num_of_visits_to_the_settings_page_per_session        0.052950\n",
       "15                       avg_num_of_visits_to_upgrade_page       -0.047196\n",
       "16                        avg_number_of_errors_per_session       -1.506800\n",
       "17                  avg_number_of_visits_to_downgrade_page        1.859356\n",
       "18                                                  gender        0.296546"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_coefficient = pd.DataFrame.from_dict({'feature':all_features,'LR coefficient': lr.bestModel.coefficients.values})\n",
    "feature_coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec1733e",
   "metadata": {},
   "source": [
    "From the above table we get an idea about how each feature contributed to the decision of the algorithm.\n",
    "The 5 most influential features were:\n",
    "- avg_num_of_adverts_per_session\n",
    "- avg_number_of_visits_to_downgrade_page\n",
    "- avg_num_of_thumbs_down_per_session\n",
    "- avg_number_of_errors_per_session\n",
    "- avg_num_of_thumbs_up_per_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4c12d52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the logistic regression model as the final stage\n",
    "pipeline_stages.append(LogisticRegression(maxIter=100, regParam=0.01, threshold=0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a38b56",
   "metadata": {},
   "source": [
    "Transform and evaluate performance on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e1221b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUCROC score on test data:  0.5144596651445966\n"
     ]
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "training_pipeline = Pipeline(stages=pipeline_stages)\n",
    "training_pipeline_model = training_pipeline.fit(train_data)\n",
    "\n",
    "transformed_test_data = training_pipeline_model.transform(test_data)\n",
    "\n",
    "print(\"AUCROC score on test data: \", evaluator.evaluate(transformed_test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba14598b",
   "metadata": {},
   "source": [
    "The performance of the model is poor. The model has no separation capacity at all. \n",
    "\n",
    "There are a few reasons for this:\n",
    "- The imbalanced data means that the model does not have enough information to learn the characteristics of churners and non-churners. There are several ways to improve this situation like upsampling, downsampling and SMOTE.\n",
    "- The features we engineered were not very helpful to the algorithm. We would have to go back to the drawing board and think of other features. There could be other features that can be engineered using the timestamp which haven't done in this project.\n",
    "- This subset of the full dataset is too small for the model to learn anything useful. We would have to train the model on the full set before we can draws any conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecbef6e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In part 2 of this project, we attempted to use ML algorithms to model the data in hand which was gathered in part 1. The aim was to use the model to predict user churn using the various features engineered from the user event logs data.\n",
    "\n",
    "We trained different base models on the training data and evaluated their performance using cross validation based on the AUC-ROC score. The logistic regression algorithm had the best score and as a result we tried to tune parameters to improve the performance. \n",
    "\n",
    "The tuned model only performed slighlty better. And finally we evaluated the performance of the tuned model on the test dataset. The model performed poorly and we discussed possible reasons and actions that can be taken to improve further on.\n",
    "\n",
    "Afterall, we can conclude that Data Science is an iterative process and not all projects are successful. However, with each project we learn to take better decisions and improve our skills. The bigger picture of this project was to learn PySpark while executing the data science process on a real business problem and I believe I have learned a great deal about it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
